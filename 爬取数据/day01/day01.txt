王伟超
  wangweichao@tedu.cn
1、概述
  1、网络爬虫(网络蜘蛛、网络机器人)
    1、定义 ：抓取网络数据的程序
    2、用python程序模仿人点击浏览器访问网站
    3、目的 ：获取大量数据分析
  2、企业获取数据的方式
    1、公司自有数据
    2、第三方数据平台购买
    3、爬虫爬取数据:市场上没有,价格太高
  3、Python做爬虫优势
    请求模块、解析模块丰富成熟,强大的Scrapy爬虫框架
    PHP ：对多线程、异步支持不太好
    JAVA：代码笨重,代码量大
    C/C++：虽然效率高,但是代码成型慢
  4、爬虫分类
    1、通用网络爬虫(搜索引擎,遵守robots协议)
      robots协议：网站通过Robots协议告诉搜索引擎哪些页面可抓,哪些不可抓
      https://www.taobao.com/robots.txt
    2、聚焦网络爬虫
      自己写的爬虫程序
  5、爬取数据步骤
    1、确定URL地址
    2、发请求,获取响应
    3、解析响应
       1、所需数据,保存
       2、页面中新的URL,继续第2步
2、WEB回顾
  1、URL ：统一资源定位符
    scheme://host[:port]/path/.../[?query-string][#anchor]
     协议    域名 端口  资源路径   查询参数      锚点
    https://item.jd.com/11936238.html#detail
    https://www.baidu.com/s?wd=核心编程&pn=10
    # 多个查询参数之间要用 & 做分隔
    query-string ：查询参数
    anchor       ：锚点(跳转到网页指定位置)
  2、HTTP和HTTPS
     HTTP ：80
     HTTS ：443(HTTP+SSL),HTTP升级版,加了SSL安全套接层,在传输层对数据加密,保障数据传输安全
  3、请求头(Request Headers)
    # 接收数据类型
    Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3
    # 是否支持压缩/解压缩
    Accept-Encoding: gzip, deflate, br
    # 支持的语言
    Accept-Language: zh-CN,zh;q=0.9
    # 缓存控制
      # max-age > 0 ：直接从浏览器缓存中提取
      # max-age <= 0：向服务器发请求确认,该资源是否修改
    Cache-Control: max-age=0
    # 支持长连接
    Connection: keep-alive
    **# 服务器可能检查
    Cookie: 
    Host: www.baidu.com
    # 升级HTTPS请求
    Upgrade-Insecure-Requests: 1
    **# 浏览器信息
    User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36
  4、GET和POST
    1、GET ：查询参数在URL地址上显示出来
    2、POST：Form表单提交,传输大文本,数据隐藏在Form表单
3、请求模块(urllib.request)
  1、urllib.request.urlopen('URL地址')
    1、作用 ：向网站发起请求并获取响应对象
       字节流 = response.read()
       字符串 = response.read().decode('utf-8')
       encode() ：字符串->bytes
       decode() ：bytes ->字符串
    2、不支持重构User-Agent
  2、urllib.request.Request('URL',headers={})
    1、创建请求对象
      request = urllib.request.Request('URL',...)
    2、发请求获取响应对象
      response = urllib.request.urlopen(request)
    3、获取响应内容
      html = response.read().decode('utf-8')
  3、响应对象(response)方法
    1、read()
    2、getcode() ：返回HTTP响应码
       200 ：成功
       302 ：临时转移至新的URL
       404 ：页面未找到
       500 ：服务器出错
4、编码模块(urllib.parse)
  1、urllib.parse.urlencode({})
     key = {'wd':'美女'}
     示例(见03_urlencode.py) ：在终端输入搜索内容,得到搜索结果,保存到本地文件
  2、urllib.parse.quote('字符串')
     string = urllib.parse.quote('冯绍峰')
     url = 'http://www.baidu.com/s?wd={}'.format(string)
  3、urllib.parse.unquote('%E8%D5%A4...')
5、urllib、urllib2、urllib库的关系
  1、Python2中
     urllib  ：URL编码  urllib.urlencode({})
     urllib2 ：发请求获响应
  2、Python3中
     将2中的urllib和urllib2合并,统称为urllib库
6、案例 ：百度贴吧数据抓取
   见：05_百度贴吧数据抓取类.py
  1、要求：
    1、输入抓取的贴吧名称
    2、起始页
    3、终止页
    4、保存到本地 ：第1页.html 第2页.html ...
  2、步骤
    1、找URL规律
       第1页：http://tieba.baidu.com/f?kw=??&pn=0
       第2页：http://tieba.baidu.com/f?kw=??&pn=50
       第n页：pn=(n-1)*50
    2、获取网页内容(发请求获响应)
    3、保存(本地、数据库)
7、正则解析模块(re)
  1、re模块使用流程
    1、写法1
      r_list = re.findall('正则','字符串',re.S)
    2、写法2
      1、创建编译对象
         p = re.compile('正则表达式',re.S)
      2、进行字符串匹配
         r_list = p.findall(html)
  2、元字符
     .  ：匹配任意一个字符(不包括\n) re.S
     \d ：1个数字
     \s ：空白字符
     \S ：非空白字符 # [\s\S]
     [] ：包含[]内容
     \w ：普通字符
     \W ：特殊字符

     *  ：0次或多次
     +  ：1次或多次
     ?  ：0次或1次
     {m}：m次
  3、贪婪匹配和非贪婪匹配
     见：06_贪婪匹配与非贪婪匹配示例
    1、贪婪匹配(.*)：在整个表达式匹配成功的前提下,尽可能多的去匹配*
    2、非贪婪匹配(.*?)：在整个表达式匹配成功的前提下,尽可能少的去匹配*
  6、正则表达式分组
    
  
作业：
  1、把百度贴吧案例重新写一遍,不要参照课上代码
  2、爬取猫眼电影信息：猫眼电影 - 榜单 - top100榜
     猫眼电影-第1页.html
     猫眼电影-第2页.html
     ...
  3、正则回顾、pymysql回顾、pymongo回顾、MySQL和MongoDB基本命令回顾
  



    


  
     






















